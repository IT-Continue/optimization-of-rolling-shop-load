{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMhlOff//fQYZOi8fOxEH9k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"9WJFBke9vJvS","executionInfo":{"status":"error","timestamp":1700842488103,"user_tz":-420,"elapsed":557,"user":{"displayName":"Mr Madmer","userId":"13901167348868443691"}},"outputId":"d528750a-40cd-44dd-8a28-067fcd4d752c"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-0c8de75650f3>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-42-0c8de75650f3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Data generators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtrain_data_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeseriesGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_ovens_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_series_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mvalidation_data_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeseriesGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_ovens_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_series_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtest_data_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeseriesGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ovens_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_series_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/sequence.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, targets, length, sampling_rate, stride, start_index, end_index, shuffle, reverse, batch_size)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    139\u001b[0m                 \u001b[0;34m\"Data and targets have to be\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34mf\" of same length. Data length is {len(data)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Data and targets have to be of same length. Data length is 7 while target length is 174"]}],"source":["import pandas as pd\n","import tensorflow as tf\n","\n","def preprocess_data(data, name=[\"name\"]):\n","    data_processed = data.drop(\"Unnamed: 0\", axis=1)\n","    # data_processed = data.drop(name[0], axis=1)\n","    data_processed = data_processed.fillna(0)\n","    # Text to binary\n","    data_processed = pd.get_dummies(data_processed, columns=name)\n","\n","    return data_processed.values\n","\n","def main():\n","    # Get data\n","    ovens_data = pd.read_csv(\"ovens_done.csv\")\n","    series_data = pd.read_csv(\"series_done.csv\")\n","\n","    # Preprocess\n","    processed_ovens_data = preprocess_data(ovens_data)\n","    processed_series_data = preprocess_data(series_data, [\"№слитка-индекс-часть\"])\n","\n","    # Make valid and test data\n","    split_index = len(processed_series_data) // 2\n","\n","    validation_ovens_data = processed_ovens_data[:split_index]\n","    validation_series_data = processed_series_data[:split_index]\n","\n","    test_ovens_data = processed_ovens_data[split_index:]\n","    test_series_data = processed_series_data[split_index:]\n","\n","    batch_size = 32\n","    sequence_length =\n","\n","    # Data generators\n","    train_data_generator = tf.keras.preprocessing.sequence.TimeseriesGenerator(processed_ovens_data, processed_series_data, length=sequence_length, batch_size=batch_size)\n","    validation_data_generator = tf.keras.preprocessing.sequence.TimeseriesGenerator(validation_ovens_data, validation_series_data, length=sequence_length, batch_size=batch_size)\n","    test_data_generator = tf.keras.preprocessing.sequence.TimeseriesGenerator(test_ovens_data, test_series_data, length=sequence_length, batch_size=batch_size)\n","\n","    # Neural network\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.LSTM(128, return_sequences=True, input_shape=(sequence_length, processed_ovens_data.shape[1])),\n","        tf.keras.layers.Dropout(0.2),\n","        tf.keras.layers.LSTM(64, return_sequences=True),\n","        tf.keras.layers.Dropout(0.2),\n","        tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(processed_series_data.shape[1], activation='softmax'))\n","    ])\n","\n","    # Model compile\n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    # Model fit\n","    model.fit(train_data_generator, epochs=10, validation_data=validation_data_generator)\n","\n","    # Result\n","    loss, accuracy = model.evaluate(test_data_generator)\n","    print(f\"Точность модели на тестовой выборке: {accuracy}\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}]}